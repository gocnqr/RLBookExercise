
\documentclass[10pt,letterpaper]{article}



% Some useful packages
% math
\usepackage{amsmath}
\usepackage{amsfonts}
% pretty colors
\usepackage{color}
% nicer urls that break at the end of the page
\usepackage{url}
% every document needs images
\usepackage{graphicx}
\usepackage{setspace}

\newcommand{\e}[1]{\mathbb E}
\newcommand\given[1][]{\:#1\vert\:}

\graphicspath{{./figures/}}   % where to look for images

%let's fiddle with the default margins to save some trees
%this makes the odd side margin go to the default of 1inch
\oddsidemargin 0.0in
%sets the textwidth to 6.5, which leaves 1 for the remaining right margin with 8 1/2X11inch paper
\textwidth 6.5in
% less white space, please!
\headheight 0.0in
% shift everything up
\topmargin -0.5in
\footskip .6in
% text should take up all but a 1'' margin
\textheight 9.0in

% Define some shortcuts for things I want to use.
% Use them like, for example:
% \begin{hypothesis}Lettuce causes brain damage.\end{hypothesis}
% Numbering & formatting will happen automatically.
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{task}{Task}
\newtheorem{contribution}{Contribution}


% Shortcuts: allows you to use limited markup when editing/collaborating.
% \comment{This section needs to be rewritten.}
\def\ask#1{\textcolor{red}{\bf $\langle\langle$Question:\ #1$\rangle\rangle$}}
\def\comment#1{\textcolor{red}{\bf $\langle\langle$Comment:\ #1$\rangle\rangle$}}


% This imitates the Wikipedia ``Citation Needed'' text; use it as a temporary
% marker for things you need to cite.
\def\citationneeded{$^{\textcolor{blue}{\text{[citation needed]}}}$ }

% format et al.
\def\etal{\textit{et al.}}
\def\ie{\textit{i.e.}}
\def\eg{\textit{e.g.}}

\title{Attempts to exercise in Reinforcement Learning book Chapter 4}
\author{Mengliao Wang}


\begin{document}

% Generate Title Page
\maketitle


% this dumps the abstract on a front page all by itself.

\section*{Exercise 4.1: }
\label{4.1}

According to $q_\pi(s,a) = r(s,a,s') + \gamma\sum_{s',r}p(r,s'\given s,a)v_\pi(s')$

We have $q_\pi(11,down) = -1 + v(T) = -1$.

Similarly we have $q_\pi(7, down) = -1 + v(11) = -2$.


\section*{Exercise 3.2: }
\label{3.2}

No. There are cases that a goal-directed learning task cannot be represented by this framework, especially for the tasks that are sophisticated to evaluate the goal. For example, when we try to make a good painting, we can define the agent, action, environment clearly. We can also define the state to be the pixels on the painting. However there is not a good way to determine the reward. We cannot decide how good or bad a painting is in a measurable way. 

\section*{Exercise 3.3: }
\label{3.3}

To separate the environment from the agent, the general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. Thus in this case anything outside of the car should be considered as environment, such as road, other vehicles, road block, etc. Anything that can be directly touched and manipulated inside the car should be considered as part of the agent, such as the break, steering wheel, accelerator, signals, etc. Lastly, regarding the parts on the car that are indirectly controlled by the driver, they should be classified as either agent or environment depending on the possibility that the this indirect control would not be functioning as expected. For example, if we do not take things that might make wheel losing control such as driving on the ice into consideration, then the wheels should be part of the agent as well, and vice versa.


\section*{Exercise 3.4: }
\label{3.4}

If we treat this pole-balancing task also as episodic but with the same reward, then given $K$ is the number of time steps before failure the return would be $-\gamma^K$. The return looks the same as continuing formula, but after reaching failure the time step will be reset to 0, instead of continuing at $K$+1.


\section*{Exercise 3.11: }
\label{3.11}

The two equations are: $v_\pi(s) = \mathbb{E}_\pi[q_\pi(s,a)\given S_t=s] = \sum_a \pi(a\given s)q_\pi(s,a)$.


\section*{Exercise 3.12: }
\label{3.12}

Similarily, the two equations are $q_\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma v_\pi(s') \given S_t=s, A_t=a, S_{t+1} = s'] = r(s,a,s') + \gamma\sum_{s',r}p(r,s'\given s,a)v_\pi(s')$


\section*{Exercise 3.13: }
\label{3.13}

The optimal policy would be to use putter on the green, and use driver whenever outside of the green. So the optimal value function would have -1 on the whole green, and -2 and above outside of the green, depending on the shortest distance $|S|$ from the location to the green. Formally, given the max range of a hit with driver is $m$, then we have the optimal value function:
\[v(s) = \begin{cases}
-1,& \text{if } green\\
-[ceil(\dfrac{|S|}{m}) + 1],              & \text{otherwise}
\end{cases}
\]


\section*{Exercise 3.14: }
\label{3.14}

On the green $q_*(s, putter)$ will be always -1, since we can hit the target with one put. Outside of the green within the max range of putter to the green, $q_*(s, putter)$ is -2 since we can hit green after one put. Between the contour of -2 we defined, and the put range in addition to the -2 contour in Figure 3.6 for $q_*(s, driver)$, $q_*(s, putter)$ is -3. Then between the contour of -3 we defined, and the put range in addition to the -3 contour in Figure 3.6 for $q_*(s, driver)$, $q_*(s, putter)$ is -4.



\clearpage

\end{document}
